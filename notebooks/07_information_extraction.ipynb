{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ad28e3",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\r\n",
    "# ðŸ“ Information Extraction - Vehicle Document Intelligence\r\n",
    "## Advanced OCR Pipeline for Text Detection and Recognition\r\n",
    "\r\n",
    "**System Architecture**:\r\n",
    "Existing Classification Model (93% accuracy) â†’ Document Type\r\n",
    "         â†“\r\n",
    "YOLO v9 Detection â†’ [license_plate, odometer, text_regions, damage]\r\n",
    "         â†“  \r\n",
    "Specialized OCR â†’ Text extraction per detected region\r\n",
    "         â†“\r\n",
    "Post-processing â†’ Validation, formatting, confidence scoring\r\n",
    "\r\n",
    "### Extraction Capabilities:\r\n",
    "1. **License Plate Recognition**: Alphanumeric characters from plates\r\n",
    "2. **Odometer Reading**: Digital/analog odometer values\r\n",
    "3. **Document Text**: Key information from papers/documents\r\n",
    "4. **Damage Detection**: Visual anomalies and damage indicators\r\n",
    "5. **Quality Scoring**: Confidence and reliability metrics\r\n",
    "\r\n",
    "### Technical Stack:\r\n",
    "- **Detection**: YOLOv9/YOLOv8 for object localization\r\n",
    "- **OCR**: EasyOCR + PaddleOCR for robust text recognition\r\n",
    "- **Validation**: RegEx patterns + business rules\r\n",
    "- **Integration**: Seamless pipeline with lassification model\r\n",
    "\"\"\"\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005a0d22-8f3f-4b11-a02c-2e992afe5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 15:43:56.641047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-23 15:43:56.641107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-23 15:43:56.642512: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-23 15:43:56.648786: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-23 15:43:57.184091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EasyOCR available\n",
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/edwlearn/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "âœ… YOLO available\n",
      "âœ… PaddleOCR available\n",
      "ðŸ”§ Information Extraction Environment Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OCR and Detection libraries\n",
    "try:\n",
    "    import easyocr\n",
    "    EASYOCR_AVAILABLE = True\n",
    "    print(\"âœ… EasyOCR available\")\n",
    "except ImportError:\n",
    "    EASYOCR_AVAILABLE = False\n",
    "    print(\"âš ï¸ EasyOCR not available - install with: pip install easyocr\")\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    YOLO_AVAILABLE = True\n",
    "    print(\"âœ… YOLO available\")\n",
    "except ImportError:\n",
    "    YOLO_AVAILABLE = False\n",
    "    print(\"âš ï¸ YOLO not available - install with: pip install ultralytics\")\n",
    "\n",
    "try:\n",
    "    import paddleocr\n",
    "    PADDLE_AVAILABLE = True\n",
    "    print(\"âœ… PaddleOCR available\")\n",
    "except ImportError:\n",
    "    PADDLE_AVAILABLE = False\n",
    "    print(\"âš ï¸ PaddleOCR not available - install with: pip install paddlepaddle paddleocr\")\n",
    "\n",
    "# Image processing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pytesseract\n",
    "from scipy import ndimage\n",
    "from skimage import measure, morphology\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set1\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"ðŸ”§ Information Extraction Environment Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5934853-7dca-48f7-8c1c-ebba515e0b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Information Extraction Configuration:\n",
      "Extraction Directory: ../models/extraction\n",
      "Detection Confidence: 0.5\n",
      "OCR Confidence: 0.6\n",
      "Available OCR engines: 2\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"car_plates\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "EXTRACTION_DIR = MODELS_DIR / \"extraction\"\n",
    "ANNOTATIONS_DIR = DATA_DIR / \"annotations\"\n",
    "\n",
    "# Create extraction directory\n",
    "EXTRACTION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Classification model configuration (from previous work)\n",
    "CLASS_NAMES = ['document', 'licence', 'odometer']\n",
    "CLASS_TO_INT = {'document': 0, 'licence': 1, 'odometer': 2}\n",
    "INT_TO_CLASS = {0: 'document', 1: 'licence', 2: 'odometer'}\n",
    "\n",
    "# OCR and Detection configuration\n",
    "DETECTION_CONFIDENCE = 0.5\n",
    "OCR_CONFIDENCE_THRESHOLD = 0.6\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Text patterns for validation\n",
    "TEXT_PATTERNS = {\n",
    "    'license_plate': {\n",
    "        'patterns': [\n",
    "            r'^[A-Z]{3}-\\d{3}$',  # ABC-123 format\n",
    "            r'^[A-Z]{2}\\d{4}$',   # AB1234 format\n",
    "            r'^\\d{3}[A-Z]{3}$',   # 123ABC format\n",
    "            r'^[A-Z]{1,3}\\d{1,4}[A-Z]?$'  # General format\n",
    "        ],\n",
    "        'min_length': 5,\n",
    "        'max_length': 10\n",
    "    },\n",
    "    'odometer': {\n",
    "        'patterns': [\n",
    "            r'^\\d{1,6}$',         # Pure numbers\n",
    "            r'^\\d{1,3},\\d{3}$',   # With comma separator\n",
    "            r'^\\d{1,3}\\.\\d{3}$',  # With dot separator\n",
    "            r'^\\d{1,6}\\s*(km|mi|KM|MI)?$'  # With units\n",
    "        ],\n",
    "        'min_value': 0,\n",
    "        'max_value': 999999\n",
    "    },\n",
    "    'document_text': {\n",
    "        'min_length': 2,\n",
    "        'max_length': 100,\n",
    "        'allowed_chars': r'[A-Za-z0-9\\s\\-\\.,:]'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ Information Extraction Configuration:\")\n",
    "print(f\"Extraction Directory: {EXTRACTION_DIR}\")\n",
    "print(f\"Detection Confidence: {DETECTION_CONFIDENCE}\")\n",
    "print(f\"OCR Confidence: {OCR_CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"Available OCR engines: {sum([EASYOCR_AVAILABLE, PADDLE_AVAILABLE])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddeff21b-675a-47cf-ac1f-a0f86d58975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading YOUR trained classification model...\n",
      "âŒ YOUR trained model not found. Please ensure your model is saved.\n",
      "ðŸ’¡ Expected locations:\n",
      "   - ../models/best_model.h5\n",
      "   - ../models/vehicle_document_classifier.h5\n",
      "   - ../models/final_model.h5\n",
      "   - ../models/model_checkpoint.h5\n",
      "âš ï¸ Warning: YOUR classification model not loaded.\n",
      "ðŸ”§ The OCR pipeline will work independently without classification.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”„ Loading YOUR trained classification model...\")\n",
    "\n",
    "# Load YOUR existing model (the one with 93% accuracy)\n",
    "def load_your_classification_model():\n",
    "    \"\"\"Load YOUR existing trained classification model\"\"\"\n",
    "    \n",
    "    # Try to load YOUR model from the most likely locations\n",
    "    model_paths = [\n",
    "        MODELS_DIR / \"best_model.h5\",\n",
    "        MODELS_DIR / \"vehicle_document_classifier.h5\",\n",
    "        MODELS_DIR / \"final_model.h5\",\n",
    "        MODELS_DIR / \"model_checkpoint.h5\"\n",
    "    ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        if model_path.exists():\n",
    "            try:\n",
    "                model = tf.keras.models.load_model(model_path)\n",
    "                print(f\"âœ… YOUR classification model loaded from: {model_path}\")\n",
    "                print(f\"   Model name: {model.name}\")\n",
    "                print(f\"   Parameters: {model.count_params():,}\")\n",
    "                return model\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to load {model_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(\"âŒ YOUR trained model not found. Please ensure your model is saved.\")\n",
    "    print(\"ðŸ’¡ Expected locations:\")\n",
    "    for path in model_paths:\n",
    "        print(f\"   - {path}\")\n",
    "    return None\n",
    "\n",
    "# Load YOUR classification model\n",
    "your_classification_model = load_your_classification_model()\n",
    "\n",
    "if your_classification_model is None:\n",
    "    print(\"âš ï¸ Warning: YOUR classification model not loaded.\")\n",
    "    print(\"ðŸ”§ The OCR pipeline will work independently without classification.\")\n",
    "else:\n",
    "    print(\"ðŸŽ¯ YOUR classification model ready for integration with OCR pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9719372-4c18-47c5-b3a2-6b0d26b7bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”§ Initializing OCR engines...\")\n",
    "\n",
    "class OCREngine:\n",
    "    \"\"\"Multi-engine OCR system for robust text recognition\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.engines = {}\n",
    "        self.initialize_engines()\n",
    "    \n",
    "    def initialize_engines(self):\n",
    "        \"\"\"Initialize available OCR engines\"\"\"\n",
    "        \n",
    "        # EasyOCR - Good for general text and multiple languages\n",
    "        if EASYOCR_AVAILABLE:\n",
    "            try:\n",
    "                self.engines['easyocr'] = easyocr.Reader(['en'])\n",
    "                print(\"âœ… EasyOCR initialized\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ EasyOCR initialization failed: {e}\")\n",
    "        \n",
    "        # PaddleOCR - Excellent for complex layouts\n",
    "        if PADDLE_AVAILABLE:\n",
    "            try:\n",
    "                self.engines['paddle'] = paddleocr.PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n",
    "                print(\"âœ… PaddleOCR initialized\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ PaddleOCR initialization failed: {e}\")\n",
    "        \n",
    "        # Tesseract - Fallback option\n",
    "        try:\n",
    "            # Test if tesseract is available\n",
    "            pytesseract.get_tesseract_version()\n",
    "            self.engines['tesseract'] = 'available'\n",
    "            print(\"âœ… Tesseract available\")\n",
    "        except:\n",
    "            print(\"âš ï¸ Tesseract not available\")\n",
    "        \n",
    "        print(f\"ðŸŽ¯ OCR engines initialized: {list(self.engines.keys())}\")\n",
    "    \n",
    "    def extract_text_easyocr(self, image_region):\n",
    "        \"\"\"Extract text using EasyOCR\"\"\"\n",
    "        if 'easyocr' not in self.engines:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.engines['easyocr'].readtext(image_region)\n",
    "            extracted_texts = []\n",
    "            \n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence >= OCR_CONFIDENCE_THRESHOLD:\n",
    "                    extracted_texts.append({\n",
    "                        'text': text.strip(),\n",
    "                        'confidence': float(confidence),\n",
    "                        'bbox': bbox,\n",
    "                        'engine': 'easyocr'\n",
    "                    })\n",
    "            \n",
    "            return extracted_texts\n",
    "        except Exception as e:\n",
    "            print(f\"EasyOCR error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_text_paddle(self, image_region):\n",
    "        \"\"\"Extract text using PaddleOCR\"\"\"\n",
    "        if 'paddle' not in self.engines:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.engines['paddle'].ocr(image_region, cls=True)\n",
    "            extracted_texts = []\n",
    "            \n",
    "            if results and results[0]:\n",
    "                for line in results[0]:\n",
    "                    if line and len(line) >= 2:\n",
    "                        bbox, (text, confidence) = line\n",
    "                        if confidence >= OCR_CONFIDENCE_THRESHOLD:\n",
    "                            extracted_texts.append({\n",
    "                                'text': text.strip(),\n",
    "                                'confidence': float(confidence),\n",
    "                                'bbox': bbox,\n",
    "                                'engine': 'paddle'\n",
    "                            })\n",
    "            \n",
    "            return extracted_texts\n",
    "        except Exception as e:\n",
    "            print(f\"PaddleOCR error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_text_tesseract(self, image_region):\n",
    "        \"\"\"Extract text using Tesseract\"\"\"\n",
    "        if 'tesseract' not in self.engines:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Configure Tesseract for different document types\n",
    "            config = '--oem 3 --psm 6'  # Assume uniform block of text\n",
    "            \n",
    "            text = pytesseract.image_to_string(image_region, config=config)\n",
    "            \n",
    "            if text.strip():\n",
    "                return [{\n",
    "                    'text': text.strip(),\n",
    "                    'confidence': 0.8,  # Tesseract doesn't provide confidence easily\n",
    "                    'bbox': None,\n",
    "                    'engine': 'tesseract'\n",
    "                }]\n",
    "            \n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Tesseract error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_text_multi_engine(self, image_region, engines=['easyocr', 'paddle']):\n",
    "        \"\"\"Extract text using multiple engines and combine results\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for engine in engines:\n",
    "            if engine == 'easyocr':\n",
    "                results = self.extract_text_easyocr(image_region)\n",
    "            elif engine == 'paddle':\n",
    "                results = self.extract_text_paddle(image_region)\n",
    "            elif engine == 'tesseract':\n",
    "                results = self.extract_text_tesseract(image_region)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            all_results.extend(results)\n",
    "        \n",
    "        # Combine and deduplicate results\n",
    "        return self.combine_ocr_results(all_results)\n",
    "    \n",
    "    def combine_ocr_results(self, results):\n",
    "        \"\"\"Combine results from multiple OCR engines\"\"\"\n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Group similar texts\n",
    "        combined = {}\n",
    "        \n",
    "        for result in results:\n",
    "            text = result['text'].upper().strip()\n",
    "            if text not in combined:\n",
    "                combined[text] = result\n",
    "            else:\n",
    "                # Keep result with higher confidence\n",
    "                if result['confidence'] > combined[text]['confidence']:\n",
    "                    combined[text] = result\n",
    "        \n",
    "        return list(combined.values())\n",
    "\n",
    "# Initialize OCR system\n",
    "ocr_engine = OCREngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79df52-87e6-434a-bb9b-aba2804365bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Setting up detection system...\")\n",
    "\n",
    "class VehicleDocumentDetector:\n",
    "    \"\"\"Vehicle document element detection system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.yolo_model = None\n",
    "        self.detection_classes = ['license_plate', 'odometer', 'text_region', 'damage']\n",
    "        self.initialize_detector()\n",
    "    \n",
    "    def initialize_detector(self):\n",
    "        \"\"\"Initialize YOLO detection model\"\"\"\n",
    "        \n",
    "        if YOLO_AVAILABLE:\n",
    "            try:\n",
    "                # Try to load pre-trained YOLO model\n",
    "                # Note: You might need to train a custom model for vehicle documents\n",
    "                self.yolo_model = YOLO('yolov8n.pt')  # Using nano version for speed\n",
    "                print(\"âœ… YOLO detector initialized\")\n",
    "                print(f\"   Model: YOLOv8 nano\")\n",
    "                print(f\"   Classes: {len(self.yolo_model.names)} default classes\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ YOLO initialization failed: {e}\")\n",
    "                self.yolo_model = None\n",
    "        else:\n",
    "            print(\"âš ï¸ YOLO not available, using fallback detection\")\n",
    "    \n",
    "    def detect_with_yolo(self, image):\n",
    "        \"\"\"Detect objects using YOLO\"\"\"\n",
    "        if self.yolo_model is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.yolo_model(image, conf=DETECTION_CONFIDENCE)\n",
    "            detections = []\n",
    "            \n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                if boxes is not None:\n",
    "                    for box in boxes:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        confidence = box.conf[0].cpu().numpy()\n",
    "                        class_id = int(box.cls[0].cpu().numpy())\n",
    "                        class_name = self.yolo_model.names[class_id]\n",
    "                        \n",
    "                        detections.append({\n",
    "                            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                            'confidence': float(confidence),\n",
    "                            'class': class_name,\n",
    "                            'class_id': class_id\n",
    "                        })\n",
    "            \n",
    "            return detections\n",
    "        except Exception as e:\n",
    "            print(f\"YOLO detection error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def detect_fallback_regions(self, image):\n",
    "        \"\"\"Fallback detection using traditional CV methods\"\"\"\n",
    "        \n",
    "        detections = []\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "        height, width = gray.shape\n",
    "        \n",
    "        # Simple region proposals based on image analysis\n",
    "        \n",
    "        # 1. Text regions using edge detection\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 100:  # Filter small regions\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                \n",
    "                # Classify region based on aspect ratio and position\n",
    "                aspect_ratio = w / h\n",
    "                relative_y = y / height\n",
    "                \n",
    "                if 0.2 < aspect_ratio < 8 and w > 50 and h > 20:  # Likely text\n",
    "                    region_type = 'text_region'\n",
    "                    \n",
    "                    # Specific heuristics for license plates (rectangular, specific ratio)\n",
    "                    if 2 < aspect_ratio < 6 and 0.3 < relative_y < 0.8:\n",
    "                        region_type = 'license_plate'\n",
    "                    \n",
    "                    # Odometer regions (usually in upper portion, squarish)\n",
    "                    elif 0.5 < aspect_ratio < 2 and relative_y < 0.5:\n",
    "                        region_type = 'odometer'\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'bbox': [x, y, x+w, y+h],\n",
    "                        'confidence': 0.7,  # Default confidence for fallback\n",
    "                        'class': region_type,\n",
    "                        'class_id': 0\n",
    "                    })\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def detect_elements(self, image):\n",
    "        \"\"\"Main detection method\"\"\"\n",
    "        \n",
    "        # Try YOLO first, fallback to traditional methods\n",
    "        detections = self.detect_with_yolo(image)\n",
    "        \n",
    "        if not detections:\n",
    "            detections = self.detect_fallback_regions(image)\n",
    "        \n",
    "        # Filter and clean detections\n",
    "        filtered_detections = []\n",
    "        for detection in detections:\n",
    "            bbox = detection['bbox']\n",
    "            \n",
    "            # Basic validation\n",
    "            if (bbox[2] > bbox[0] and bbox[3] > bbox[1] and \n",
    "                bbox[2] - bbox[0] > 20 and bbox[3] - bbox[1] > 20):\n",
    "                \n",
    "                filtered_detections.append(detection)\n",
    "        \n",
    "        return filtered_detections\n",
    "\n",
    "# Initialize detector\n",
    "detector = VehicleDocumentDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3736e20-c9b2-45ec-a1ca-cf0a1020e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextValidator:\n",
    "    \"\"\"Text validation and post-processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = TEXT_PATTERNS\n",
    "    \n",
    "    def validate_license_plate(self, text):\n",
    "        \"\"\"Validate license plate format\"\"\"\n",
    "        text = text.upper().strip().replace(' ', '').replace('-', '')\n",
    "        \n",
    "        # Check length\n",
    "        if not (self.patterns['license_plate']['min_length'] <= \n",
    "                len(text) <= self.patterns['license_plate']['max_length']):\n",
    "            return False, 0.0\n",
    "        \n",
    "        # Check patterns\n",
    "        for pattern in self.patterns['license_plate']['patterns']:\n",
    "            modified_pattern = pattern.replace('-', '')\n",
    "            if re.match(modified_pattern, text):\n",
    "                return True, 0.9\n",
    "        \n",
    "        # Partial match scoring\n",
    "        alphanumeric_ratio = sum(c.isalnum() for c in text) / len(text)\n",
    "        if alphanumeric_ratio > 0.8:\n",
    "            return True, 0.6\n",
    "        \n",
    "        return False, 0.0\n",
    "    \n",
    "    def validate_odometer(self, text):\n",
    "        \"\"\"Validate odometer reading\"\"\"\n",
    "        # Clean text\n",
    "        cleaned = re.sub(r'[^\\d\\.,]', '', text.strip())\n",
    "        \n",
    "        if not cleaned:\n",
    "            return False, 0.0\n",
    "        \n",
    "        # Try to extract number\n",
    "        try:\n",
    "            # Handle different formats\n",
    "            if ',' in cleaned:\n",
    "                value = float(cleaned.replace(',', ''))\n",
    "            elif '.' in cleaned and cleaned.count('.') == 1:\n",
    "                value = float(cleaned)\n",
    "            else:\n",
    "                value = float(cleaned)\n",
    "            \n",
    "            # Validate range\n",
    "            if (self.patterns['odometer']['min_value'] <= \n",
    "                value <= self.patterns['odometer']['max_value']):\n",
    "                return True, 0.9\n",
    "            else:\n",
    "                return False, 0.3\n",
    "                \n",
    "        except ValueError:\n",
    "            return False, 0.0\n",
    "    \n",
    "    def validate_document_text(self, text):\n",
    "        \"\"\"Validate general document text\"\"\"\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Check length\n",
    "        if not (self.patterns['document_text']['min_length'] <= \n",
    "                len(text) <= self.patterns['document_text']['max_length']):\n",
    "            return False, 0.0\n",
    "        \n",
    "        # Check allowed characters\n",
    "        allowed_chars = self.patterns['document_text']['allowed_chars']\n",
    "        if re.match(f'^{allowed_chars}+$', text):\n",
    "            return True, 0.8\n",
    "        \n",
    "        # Partial validation\n",
    "        valid_char_ratio = len(re.findall(allowed_chars, text)) / len(text)\n",
    "        if valid_char_ratio > 0.7:\n",
    "            return True, 0.6\n",
    "        \n",
    "        return False, 0.0\n",
    "    \n",
    "    def format_license_plate(self, text):\n",
    "        \"\"\"Format license plate text\"\"\"\n",
    "        text = text.upper().strip().replace(' ', '')\n",
    "        \n",
    "        # Common formatting patterns\n",
    "        if len(text) == 6 and text[:3].isalpha() and text[3:].isdigit():\n",
    "            return f\"{text[:3]}-{text[3:]}\"\n",
    "        elif len(text) == 6 and text[:2].isalpha() and text[2:].isdigit():\n",
    "            return f\"{text[:2]}{text[2:]}\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def format_odometer(self, text):\n",
    "        \"\"\"Format odometer reading\"\"\"\n",
    "        cleaned = re.sub(r'[^\\d]', '', text.strip())\n",
    "        \n",
    "        if len(cleaned) > 3:\n",
    "            # Add comma separator for readability\n",
    "            return f\"{int(cleaned):,}\"\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def post_process_text(self, text, text_type):\n",
    "        \"\"\"Post-process extracted text based on type\"\"\"\n",
    "        \n",
    "        if text_type == 'license_plate':\n",
    "            is_valid, confidence = self.validate_license_plate(text)\n",
    "            formatted_text = self.format_license_plate(text) if is_valid else text\n",
    "            \n",
    "        elif text_type == 'odometer':\n",
    "            is_valid, confidence = self.validate_odometer(text)\n",
    "            formatted_text = self.format_odometer(text) if is_valid else text\n",
    "            \n",
    "        elif text_type == 'document_text':\n",
    "            is_valid, confidence = self.validate_document_text(text)\n",
    "            formatted_text = text.strip()\n",
    "            \n",
    "        else:\n",
    "            is_valid, confidence = True, 0.5\n",
    "            formatted_text = text.strip()\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'formatted_text': formatted_text,\n",
    "            'is_valid': is_valid,\n",
    "            'validation_confidence': confidence\n",
    "        }\n",
    "\n",
    "# Initialize text validator\n",
    "text_validator = TextValidator()\n",
    "\n",
    "print(\"âœ… Text validation system initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93384f09-92e2-4a42-b89e-6c9ae8a83c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VehicleDocumentExtractor:\n",
    "    \"\"\"Complete vehicle document information extraction system using YOUR trained model\"\"\"\n",
    "    \n",
    "    def __init__(self, your_model, detector, ocr_engine, text_validator):\n",
    "        self.your_model = your_model\n",
    "        self.detector = detector\n",
    "        self.ocr_engine = ocr_engine\n",
    "        self.text_validator = text_validator\n",
    "        \n",
    "    def classify_document(self, image):\n",
    "        \"\"\"Classify document type using YOUR trained model\"\"\"\n",
    "        if self.your_model is None:\n",
    "            return {'prediction': 'unknown', 'confidence': 0.0}\n",
    "        \n",
    "        try:\n",
    "            # Preprocess image for YOUR model\n",
    "            if len(image.shape) == 3:\n",
    "                processed_image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                processed_image = processed_image.astype(np.float32) / 255.0\n",
    "                processed_image = np.expand_dims(processed_image, axis=0)\n",
    "            \n",
    "            # Get prediction from YOUR model\n",
    "            prediction = self.your_model.predict(processed_image, verbose=0)\n",
    "            class_id = np.argmax(prediction[0])\n",
    "            confidence = float(prediction[0][class_id])\n",
    "            \n",
    "            return {\n",
    "                'prediction': CLASS_NAMES[class_id],\n",
    "                'confidence': confidence,\n",
    "                'probabilities': {CLASS_NAMES[i]: float(prediction[0][i]) \n",
    "                                for i in range(len(CLASS_NAMES))}\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Classification error with YOUR model: {e}\")\n",
    "            return {'prediction': 'unknown', 'confidence': 0.0}\n",
    "    \n",
    "    def extract_information(self, image_path):\n",
    "        \"\"\"Complete information extraction pipeline\"\"\"\n",
    "        \n",
    "        # Load image\n",
    "        if isinstance(image_path, str) or isinstance(image_path, Path):\n",
    "            image = cv2.imread(str(image_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            image = image_path\n",
    "        \n",
    "        results = {\n",
    "            'image_info': {\n",
    "                'shape': image.shape,\n",
    "                'path': str(image_path) if isinstance(image_path, (str, Path)) else 'array'\n",
    "            },\n",
    "            'classification': {},\n",
    "            'detections': [],\n",
    "            'extracted_text': [],\n",
    "            'processed_information': {},\n",
    "            'overall_confidence': 0.0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Classify document type\n",
    "            print(\"ðŸ” Step 1: Document classification...\")\n",
    "            classification_result = self.classify_document(image)\n",
    "            results['classification'] = classification_result\n",
    "            \n",
    "            # Step 2: Detect elements in image\n",
    "            print(\"ðŸŽ¯ Step 2: Element detection...\")\n",
    "            detections = self.detector.detect_elements(image)\n",
    "            results['detections'] = detections\n",
    "            \n",
    "            # Step 3: Extract text from detected regions\n",
    "            print(\"ðŸ“ Step 3: Text extraction...\")\n",
    "            extracted_texts = []\n",
    "            \n",
    "            for i, detection in enumerate(detections):\n",
    "                bbox = detection['bbox']\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                \n",
    "                # Extract region\n",
    "                region = image[y1:y2, x1:x2]\n",
    "                \n",
    "                if region.size > 0:\n",
    "                    # OCR on region\n",
    "                    ocr_results = self.ocr_engine.extract_text_multi_engine(region)\n",
    "                    \n",
    "                    for ocr_result in ocr_results:\n",
    "                        # Determine text type based on detection class\n",
    "                        detected_class = detection['class']\n",
    "                        if 'plate' in detected_class.lower():\n",
    "                            text_type = 'license_plate'\n",
    "                        elif 'odometer' in detected_class.lower():\n",
    "                            text_type = 'odometer'\n",
    "                        else:\n",
    "                            text_type = 'document_text'\n",
    "                        \n",
    "                        # Post-process text\n",
    "                        processed = self.text_validator.post_process_text(\n",
    "                            ocr_result['text'], text_type\n",
    "                        )\n",
    "                        \n",
    "                        extracted_text_info = {\n",
    "                            'detection_id': i,\n",
    "                            'region_bbox': bbox,\n",
    "                            'region_class': detected_class,\n",
    "                            'text_type': text_type,\n",
    "                            'raw_text': ocr_result['text'],\n",
    "                            'processed_text': processed,\n",
    "                            'ocr_confidence': ocr_result['confidence'],\n",
    "                            'ocr_engine': ocr_result['engine'],\n",
    "                            'text_bbox': ocr_result.get('bbox', None)\n",
    "                        }\n",
    "                        \n",
    "                        extracted_texts.append(extracted_text_info)\n",
    "            \n",
    "            results['extracted_text'] = extracted_texts\n",
    "            \n",
    "            # Step 4: Structure and validate information\n",
    "            print(\"ðŸ§¹ Step 4: Information structuring...\")\n",
    "            structured_info = self.structure_information(extracted_texts, classification_result)\n",
    "            results['processed_information'] = structured_info\n",
    "            \n",
    "            # Step 5: Calculate overall confidence\n",
    "            results['overall_confidence'] = self.calculate_overall_confidence(results)\n",
    "            \n",
    "            print(f\"âœ… Extraction complete! Found {len(extracted_texts)} text elements\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Extraction pipeline error: {e}\")\n",
    "            results['error'] = str(e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def structure_information(self, extracted_texts, classification_result):\n",
    "        \"\"\"Structure extracted information by type\"\"\"\n",
    "        \n",
    "        structured = {\n",
    "            'license_plates': [],\n",
    "            'odometer_readings': [],\n",
    "            'document_texts': [],\n",
    "            'summary': {\n",
    "                'document_type': classification_result.get('prediction', 'unknown'),\n",
    "                'classification_confidence': classification_result.get('confidence', 0.0),\n",
    "                'total_extractions': len(extracted_texts),\n",
    "                'high_confidence_extractions': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for text_info in extracted_texts:\n",
    "            processed = text_info['processed_text']\n",
    "            \n",
    "            # Create structured entry\n",
    "            entry = {\n",
    "                'text': processed['formatted_text'],\n",
    "                'raw_text': text_info['raw_text'],\n",
    "                'confidence': text_info['ocr_confidence'],\n",
    "                'validation_confidence': processed['validation_confidence'],\n",
    "                'combined_confidence': (text_info['ocr_confidence'] + processed['validation_confidence']) / 2,\n",
    "                'is_valid': processed['is_valid'],\n",
    "                'region_bbox': text_info['region_bbox'],\n",
    "                'ocr_engine': text_info['ocr_engine']\n",
    "            }\n",
    "            \n",
    "            # Categorize by type\n",
    "            if text_info['text_type'] == 'license_plate':\n",
    "                structured['license_plates'].append(entry)\n",
    "            elif text_info['text_type'] == 'odometer':\n",
    "                structured['odometer_readings'].append(entry)\n",
    "            else:\n",
    "                structured['document_texts'].append(entry)\n",
    "            \n",
    "            # Count high confidence extractions\n",
    "            if entry['combined_confidence'] > 0.7:\n",
    "                structured['summary']['high_confidence_extractions'] += 1\n",
    "        \n",
    "        return structured\n",
    "    \n",
    "    def calculate_overall_confidence(self, results):\n",
    "        \"\"\"Calculate overall extraction confidence\"\"\"\n",
    "        \n",
    "        confidences = []\n",
    "        \n",
    "        # Classification confidence\n",
    "        if 'classification' in results and 'confidence' in results['classification']:\n",
    "            confidences.append(results['classification']['confidence'])\n",
    "        \n",
    "        # Text extraction confidences\n",
    "        for text_info in results.get('extracted_text', []):\n",
    "            if text_info['processed_text']['is_valid']:\n",
    "                combined_conf = (text_info['ocr_confidence'] + \n",
    "                               text_info['processed_text']['validation_confidence']) / 2\n",
    "                confidences.append(combined_conf)\n",
    "        \n",
    "        return np.mean(confidences) if confidences else 0.0\n",
    "\n",
    "# Initialize complete extraction system using YOUR model\n",
    "if your_classification_model and detector and ocr_engine and text_validator:\n",
    "    extractor = VehicleDocumentExtractor(\n",
    "        your_model=your_classification_model,\n",
    "        detector=detector,\n",
    "        ocr_engine=ocr_engine,\n",
    "        text_validator=text_validator\n",
    "    )\n",
    "    print(\"ðŸš€ Complete extraction system initialized with YOUR trained model!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some components missing:\")\n",
    "    print(f\"   YOUR model: {'âœ…' if your_classification_model else 'âŒ'}\")\n",
    "    print(f\"   Detector: {'âœ…' if detector else 'âŒ'}\")\n",
    "    print(f\"   OCR engine: {'âœ…' if ocr_engine else 'âŒ'}\")\n",
    "    print(f\"   Text validator: {'âœ…' if text_validator else 'âŒ'}\")\n",
    "\n",
    "print(\"\\nâœ… Information extraction pipeline ready!\")\n",
    "print(\"ðŸŽ¯ Ready to process vehicle documents using YOUR model + YOLO + OCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e06a27-3aaf-4a2f-ac1f-f3c36f0fed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ§ª TESTING INFORMATION EXTRACTION PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def load_test_annotations():\n",
    "    \"\"\"Load test annotations for evaluation\"\"\"\n",
    "    try:\n",
    "        with open(ANNOTATIONS_DIR / \"test_balanced_final.json\", 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        print(f\"ðŸ“Š Loaded {len(test_data)} test samples\")\n",
    "        return test_data\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not load test annotations: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load test data\n",
    "test_annotations = load_test_annotations()\n",
    "\n",
    "# Process sample images\n",
    "if test_annotations and len(test_annotations) > 0:\n",
    "    print(\"\\nðŸ”„ Processing sample images...\")\n",
    "    \n",
    "    # Select diverse samples for testing\n",
    "    sample_indices = [0, len(test_annotations)//4, len(test_annotations)//2, -1]\n",
    "    sample_results = []\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices[:3]):  # Process first 3 samples\n",
    "        if idx < len(test_annotations):\n",
    "            annotation = test_annotations[idx]\n",
    "            \n",
    "            # Get image path with fallback strategy\n",
    "            image_path = None\n",
    "            for path_key in ['enhanced_path', 'roi_path', 'original_path']:\n",
    "                if path_key in annotation:\n",
    "                    potential_path = annotation[path_key]\n",
    "                    \n",
    "                    if Path(potential_path).exists():\n",
    "                        image_path = potential_path\n",
    "                        break\n",
    "                    elif Path(PROJECT_ROOT / potential_path).exists():\n",
    "                        image_path = PROJECT_ROOT / potential_path\n",
    "                        break\n",
    "            \n",
    "            if image_path and Path(image_path).exists():\n",
    "                print(f\"\\nðŸ“¸ Processing sample {i+1}: {Path(image_path).name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Extract information\n",
    "                    if 'extractor' in locals():\n",
    "                        result = extractor.extract_information(image_path)\n",
    "                        result['sample_info'] = {\n",
    "                            'index': idx,\n",
    "                            'filename': Path(image_path).name,\n",
    "                            'annotation': annotation\n",
    "                        }\n",
    "                        sample_results.append(result)\n",
    "                        \n",
    "                        # Print summary\n",
    "                        print(f\"   Document type: {result['classification'].get('prediction', 'unknown')} \"\n",
    "                              f\"({result['classification'].get('confidence', 0):.3f})\")\n",
    "                        print(f\"   Detections: {len(result['detections'])}\")\n",
    "                        print(f\"   Text extractions: {len(result['extracted_text'])}\")\n",
    "                        print(f\"   Overall confidence: {result['overall_confidence']:.3f}\")\n",
    "                        \n",
    "                        # Show extracted texts\n",
    "                        structured = result['processed_information']\n",
    "                        if structured['license_plates']:\n",
    "                            for plate in structured['license_plates']:\n",
    "                                print(f\"   ðŸ“‹ License plate: '{plate['text']}' (conf: {plate['combined_confidence']:.3f})\")\n",
    "                        \n",
    "                        if structured['odometer_readings']:\n",
    "                            for odo in structured['odometer_readings']:\n",
    "                                print(f\"   ðŸ”¢ Odometer: '{odo['text']}' (conf: {odo['combined_confidence']:.3f})\")\n",
    "                        \n",
    "                        if structured['document_texts']:\n",
    "                            for doc in structured['document_texts'][:2]:  # Show first 2\n",
    "                                print(f\"   ðŸ“„ Document text: '{doc['text'][:50]}...' (conf: {doc['combined_confidence']:.3f})\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"   âš ï¸ Extractor not available\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error processing sample: {e}\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ Sample {i+1}: Image not found\")\n",
    "    \n",
    "    print(f\"\\nâœ… Processed {len(sample_results)} samples successfully\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No test data available for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d94eac-dce6-4ff7-97b5-46a5575d04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Creating extraction result visualizations...\")\n",
    "\n",
    "def visualize_extraction_result(result, save_path=None):\n",
    "    \"\"\"Visualize extraction results with bounding boxes and text\"\"\"\n",
    "    \n",
    "    if 'sample_info' not in result:\n",
    "        print(\"âš ï¸ No image info available for visualization\")\n",
    "        return None\n",
    "    \n",
    "    # Load original image\n",
    "    image_path = None\n",
    "    annotation = result['sample_info']['annotation']\n",
    "    \n",
    "    for path_key in ['enhanced_path', 'roi_path', 'original_path']:\n",
    "        if path_key in annotation:\n",
    "            potential_path = annotation[path_key]\n",
    "            if Path(potential_path).exists():\n",
    "                image_path = potential_path\n",
    "                break\n",
    "            elif Path(PROJECT_ROOT / potential_path).exists():\n",
    "                image_path = PROJECT_ROOT / potential_path\n",
    "                break\n",
    "    \n",
    "    if not image_path:\n",
    "        print(\"âš ï¸ Cannot load image for visualization\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare image\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle(f'ðŸ” Extraction Results: {Path(image_path).name}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Original image\n",
    "        axes[0, 0].imshow(image)\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # 2. Detection results\n",
    "        image_with_detections = image.copy()\n",
    "        \n",
    "        # Draw detection bounding boxes\n",
    "        for i, detection in enumerate(result['detections']):\n",
    "            bbox = detection['bbox']\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            \n",
    "            # Color by detection type\n",
    "            color_map = {\n",
    "                'license_plate': (255, 0, 0),    # Red\n",
    "                'odometer': (0, 255, 0),         # Green\n",
    "                'text_region': (0, 0, 255),      # Blue\n",
    "                'damage': (255, 255, 0)          # Yellow\n",
    "            }\n",
    "            \n",
    "            color = color_map.get(detection['class'], (128, 128, 128))\n",
    "            \n",
    "            cv2.rectangle(image_with_detections, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"{detection['class']} ({detection['confidence']:.2f})\"\n",
    "            cv2.putText(image_with_detections, label, (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        \n",
    "        axes[0, 1].imshow(image_with_detections)\n",
    "        axes[0, 1].set_title(f'Detections ({len(result[\"detections\"])})')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # 3. Extracted text regions\n",
    "        if result['extracted_text']:\n",
    "            # Show first few text regions\n",
    "            text_mosaic = []\n",
    "            for i, text_info in enumerate(result['extracted_text'][:6]):  # Max 6 regions\n",
    "                bbox = text_info['region_bbox']\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                region = image[y1:y2, x1:x2]\n",
    "                \n",
    "                if region.size > 0:\n",
    "                    # Resize region for display\n",
    "                    region_resized = cv2.resize(region, (100, 50))\n",
    "                    text_mosaic.append(region_resized)\n",
    "            \n",
    "            if text_mosaic:\n",
    "                # Arrange in grid\n",
    "                if len(text_mosaic) >= 4:\n",
    "                    top_row = np.hstack(text_mosaic[:2])\n",
    "                    bottom_row = np.hstack(text_mosaic[2:4])\n",
    "                    mosaic = np.vstack([top_row, bottom_row])\n",
    "                elif len(text_mosaic) >= 2:\n",
    "                    mosaic = np.hstack(text_mosaic[:2])\n",
    "                else:\n",
    "                    mosaic = text_mosaic[0]\n",
    "                \n",
    "                axes[1, 0].imshow(mosaic)\n",
    "                axes[1, 0].set_title('Extracted Text Regions')\n",
    "                axes[1, 0].axis('off')\n",
    "            else:\n",
    "                axes[1, 0].text(0.5, 0.5, 'No text regions\\nextracted', \n",
    "                               ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "                axes[1, 0].axis('off')\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No text regions\\nextracted', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].axis('off')\n",
    "        \n",
    "        # 4. Results summary\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        # Create text summary\n",
    "        summary_text = []\n",
    "        summary_text.append(f\"ðŸ“‹ EXTRACTION SUMMARY\")\n",
    "        summary_text.append(f\"\")\n",
    "        \n",
    "        # Classification\n",
    "        classification = result['classification']\n",
    "        summary_text.append(f\"Document Type: {classification.get('prediction', 'Unknown')}\")\n",
    "        summary_text.append(f\"Classification Confidence: {classification.get('confidence', 0):.3f}\")\n",
    "        summary_text.append(f\"\")\n",
    "        \n",
    "        # Extracted information\n",
    "        structured = result['processed_information']\n",
    "        \n",
    "        if structured['license_plates']:\n",
    "            summary_text.append(f\"ðŸš— License Plates:\")\n",
    "            for plate in structured['license_plates']:\n",
    "                summary_text.append(f\"  â€¢ {plate['text']} (conf: {plate['combined_confidence']:.3f})\")\n",
    "        \n",
    "        if structured['odometer_readings']:\n",
    "            summary_text.append(f\"ðŸ”¢ Odometer Readings:\")\n",
    "            for odo in structured['odometer_readings']:\n",
    "                summary_text.append(f\"  â€¢ {odo['text']} (conf: {odo['combined_confidence']:.3f})\")\n",
    "        \n",
    "        if structured['document_texts']:\n",
    "            summary_text.append(f\"ðŸ“„ Document Texts:\")\n",
    "            for doc in structured['document_texts'][:3]:  # Show first 3\n",
    "                text_preview = doc['text'][:30] + \"...\" if len(doc['text']) > 30 else doc['text']\n",
    "                summary_text.append(f\"  â€¢ {text_preview} (conf: {doc['combined_confidence']:.3f})\")\n",
    "        \n",
    "        summary_text.append(f\"\")\n",
    "        summary_text.append(f\"Overall Confidence: {result['overall_confidence']:.3f}\")\n",
    "        \n",
    "        # Display summary text\n",
    "        summary_str = '\\n'.join(summary_text)\n",
    "        axes[1, 1].text(0.05, 0.95, summary_str, transform=axes[1, 1].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ðŸ“¸ Visualization saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Visualization error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Visualize results for processed samples\n",
    "if sample_results:\n",
    "    print(\"\\nðŸŽ¨ Creating visualizations for processed samples...\")\n",
    "    \n",
    "    for i, result in enumerate(sample_results[:2]):  # Visualize first 2 samples\n",
    "        save_path = EXTRACTION_DIR / f\"extraction_result_{i+1}.png\"\n",
    "        print(f\"\\nðŸ“Š Visualizing sample {i+1}...\")\n",
    "        visualize_extraction_result(result, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efee02-0c68-4e99-aaad-1b699979d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š EXTRACTION PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_extraction_performance(sample_results):\n",
    "    \"\"\"Evaluate extraction pipeline performance\"\"\"\n",
    "    \n",
    "    if not sample_results:\n",
    "        print(\"âš ï¸ No results to evaluate\")\n",
    "        return {}\n",
    "    \n",
    "    metrics = {\n",
    "        'total_samples': len(sample_results),\n",
    "        'successful_classifications': 0,\n",
    "        'successful_detections': 0,\n",
    "        'successful_extractions': 0,\n",
    "        'high_confidence_extractions': 0,\n",
    "        'average_confidence': 0.0,\n",
    "        'extraction_breakdown': {\n",
    "            'license_plates': 0,\n",
    "            'odometer_readings': 0,\n",
    "            'document_texts': 0\n",
    "        },\n",
    "        'confidence_distribution': [],\n",
    "        'processing_success_rate': 0.0\n",
    "    }\n",
    "    \n",
    "    total_confidence = 0.0\n",
    "    \n",
    "    for result in sample_results:\n",
    "        # Check if processing was successful\n",
    "        if 'error' not in result:\n",
    "            metrics['successful_detections'] += 1\n",
    "            \n",
    "            # Classification success\n",
    "            if result['classification'].get('confidence', 0) > 0.5:\n",
    "                metrics['successful_classifications'] += 1\n",
    "            \n",
    "            # Extraction success\n",
    "            if result['extracted_text']:\n",
    "                metrics['successful_extractions'] += 1\n",
    "            \n",
    "            # Overall confidence\n",
    "            overall_conf = result['overall_confidence']\n",
    "            total_confidence += overall_conf\n",
    "            metrics['confidence_distribution'].append(overall_conf)\n",
    "            \n",
    "            if overall_conf > 0.7:\n",
    "                metrics['high_confidence_extractions'] += 1\n",
    "            \n",
    "            # Breakdown by type\n",
    "            structured = result['processed_information']\n",
    "            metrics['extraction_breakdown']['license_plates'] += len(structured['license_plates'])\n",
    "            metrics['extraction_breakdown']['odometer_readings'] += len(structured['odometer_readings'])\n",
    "            metrics['extraction_breakdown']['document_texts'] += len(structured['document_texts'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    if metrics['total_samples'] > 0:\n",
    "        metrics['average_confidence'] = total_confidence / metrics['total_samples']\n",
    "        metrics['processing_success_rate'] = metrics['successful_detections'] / metrics['total_samples']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate performance\n",
    "if sample_results:\n",
    "    performance_metrics = evaluate_extraction_performance(sample_results)\n",
    "    \n",
    "    print(\"ðŸ“ˆ EXTRACTION PIPELINE PERFORMANCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total samples processed: {performance_metrics['total_samples']}\")\n",
    "    print(f\"Processing success rate: {performance_metrics['processing_success_rate']:.1%}\")\n",
    "    print(f\"Classification success: {performance_metrics['successful_classifications']}/{performance_metrics['total_samples']}\")\n",
    "    print(f\"Detection success: {performance_metrics['successful_detections']}/{performance_metrics['total_samples']}\")\n",
    "    print(f\"Extraction success: {performance_metrics['successful_extractions']}/{performance_metrics['total_samples']}\")\n",
    "    print(f\"High confidence extractions: {performance_metrics['high_confidence_extractions']}/{performance_metrics['total_samples']}\")\n",
    "    print(f\"Average confidence: {performance_metrics['average_confidence']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š EXTRACTION BREAKDOWN:\")\n",
    "    breakdown = performance_metrics['extraction_breakdown']\n",
    "    print(f\"License plates found: {breakdown['license_plates']}\")\n",
    "    print(f\"Odometer readings found: {breakdown['odometer_readings']}\")\n",
    "    print(f\"Document texts found: {breakdown['document_texts']}\")\n",
    "    print(f\"Total extractions: {sum(breakdown.values())}\")\n",
    "    \n",
    "    # Create performance visualization\n",
    "    if len(sample_results) > 1:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('ðŸ“Š Information Extraction Performance Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Success rates\n",
    "        ax1 = axes[0, 0]\n",
    "        categories = ['Processing', 'Classification', 'Detection', 'Extraction', 'High Confidence']\n",
    "        success_rates = [\n",
    "            performance_metrics['processing_success_rate'],\n",
    "            performance_metrics['successful_classifications'] / performance_metrics['total_samples'],\n",
    "            performance_metrics['successful_detections'] / performance_metrics['total_samples'],\n",
    "            performance_metrics['successful_extractions'] / performance_metrics['total_samples'],\n",
    "            performance_metrics['high_confidence_extractions'] / performance_metrics['total_samples']\n",
    "        ]\n",
    "        \n",
    "        bars = ax1.bar(categories, [rate * 100 for rate in success_rates], \n",
    "                      color=['skyblue', 'lightgreen', 'orange', 'gold', 'lightcoral'])\n",
    "        ax1.set_title('Success Rates by Category')\n",
    "        ax1.set_ylabel('Success Rate (%)')\n",
    "        ax1.set_ylim(0, 100)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, rate in zip(bars, success_rates):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{rate:.1%}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # 2. Extraction type breakdown\n",
    "        ax2 = axes[0, 1]\n",
    "        extraction_types = list(breakdown.keys())\n",
    "        extraction_counts = list(breakdown.values())\n",
    "        \n",
    "        if sum(extraction_counts) > 0:\n",
    "            ax2.pie(extraction_counts, labels=extraction_types, autopct='%1.1f%%',\n",
    "                   colors=['lightblue', 'lightgreen', 'lightyellow'])\n",
    "            ax2.set_title('Extraction Types Distribution')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No extractions\\nto display', ha='center', va='center')\n",
    "            ax2.set_title('Extraction Types Distribution')\n",
    "        \n",
    "        # 3. Confidence distribution\n",
    "        ax3 = axes[1, 0]\n",
    "        if performance_metrics['confidence_distribution']:\n",
    "            ax3.hist(performance_metrics['confidence_distribution'], bins=10, \n",
    "                    color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "            ax3.set_title('Confidence Score Distribution')\n",
    "            ax3.set_xlabel('Confidence Score')\n",
    "            ax3.set_ylabel('Frequency')\n",
    "            ax3.axvline(performance_metrics['average_confidence'], color='red', \n",
    "                       linestyle='--', label=f'Average: {performance_metrics[\"average_confidence\"]:.3f}')\n",
    "            ax3.legend()\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'No confidence\\ndata available', ha='center', va='center')\n",
    "            ax3.set_title('Confidence Score Distribution')\n",
    "        \n",
    "        # 4. Sample results summary\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # Create summary text\n",
    "        summary_lines = [\n",
    "            \"ðŸ“‹ EXTRACTION SUMMARY\",\n",
    "            \"\",\n",
    "            f\"âœ… Processed: {performance_metrics['total_samples']} samples\",\n",
    "            f\"ðŸŽ¯ Success Rate: {performance_metrics['processing_success_rate']:.1%}\",\n",
    "            f\"ðŸ“Š Avg Confidence: {performance_metrics['average_confidence']:.3f}\",\n",
    "            \"\",\n",
    "            f\"ðŸš— License Plates: {breakdown['license_plates']}\",\n",
    "            f\"ðŸ”¢ Odometer Readings: {breakdown['odometer_readings']}\",\n",
    "            f\"ðŸ“„ Document Texts: {breakdown['document_texts']}\",\n",
    "            \"\",\n",
    "            f\"ðŸ† High Confidence: {performance_metrics['high_confidence_extractions']}/{performance_metrics['total_samples']}\",\n",
    "            f\"âš¡ Total Extractions: {sum(breakdown.values())}\"\n",
    "        ]\n",
    "        \n",
    "        summary_text = '\\n'.join(summary_lines)\n",
    "        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
    "                fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save performance visualization\n",
    "        perf_viz_path = EXTRACTION_DIR / \"extraction_performance_analysis.png\"\n",
    "        plt.savefig(perf_viz_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"ðŸ“Š Performance analysis saved to: {perf_viz_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No results available for performance evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efde0a-6cf9-49fb-9077-223af3c49f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nðŸ’¾ Saving extraction results and configuration...\")\n",
    "\n",
    "# Save sample results\n",
    "if sample_results:\n",
    "    results_data = {\n",
    "        \"experiment_metadata\": {\n",
    "            \"experiment_type\": \"information_extraction\",\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"total_samples\": len(sample_results),\n",
    "            \"ocr_engines\": list(ocr_engine.engines.keys()) if 'ocr_engine' in locals() else [],\n",
    "            \"detection_system\": \"YOLO + CV fallback\",\n",
    "            \"validation_patterns\": TEXT_PATTERNS\n",
    "        },\n",
    "        \"performance_metrics\": performance_metrics if 'performance_metrics' in locals() else {},\n",
    "        \"sample_results\": sample_results,\n",
    "        \"system_configuration\": {\n",
    "            \"your_classification_model_available\": your_classification_model is not None,\n",
    "            \"detection_confidence_threshold\": DETECTION_CONFIDENCE,\n",
    "            \"ocr_confidence_threshold\": OCR_CONFIDENCE_THRESHOLD,\n",
    "            \"image_size\": IMG_SIZE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    results_path = EXTRACTION_DIR / \"extraction_results.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        json_data = json.loads(json.dumps(results_data, default=str))\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Extraction results saved to: {results_path}\")\n",
    "\n",
    "# Save system configuration for production\n",
    "production_config = {\n",
    "    \"pipeline_components\": {\n",
    "        \"your_classification_model\": \"integrated\" if your_classification_model else \"not_available\",\n",
    "        \"detection_system\": \"YOLO + CV fallback\",\n",
    "        \"ocr_engines\": list(ocr_engine.engines.keys()) if 'ocr_engine' in locals() else [],\n",
    "        \"text_validation\": \"pattern_based\"\n",
    "    },\n",
    "    \"processing_parameters\": {\n",
    "        \"detection_confidence\": DETECTION_CONFIDENCE,\n",
    "        \"ocr_confidence\": OCR_CONFIDENCE_THRESHOLD,\n",
    "        \"image_size\": IMG_SIZE\n",
    "    },\n",
    "    \"output_structure\": {\n",
    "        \"classification\": \"document_type + confidence\",\n",
    "        \"detections\": \"bbox + class + confidence\",\n",
    "        \"extractions\": \"text + validation + confidence\",\n",
    "        \"structured_info\": \"organized_by_type\"\n",
    "    },\n",
    "    \"validation_patterns\": TEXT_PATTERNS\n",
    "}\n",
    "\n",
    "config_path = EXTRACTION_DIR / \"production_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(production_config, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Production configuration saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All extraction artifacts saved to: {EXTRACTION_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
